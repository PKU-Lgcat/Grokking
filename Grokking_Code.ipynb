{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e0f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行该段代码以防止在训练过程中内核挂掉\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053b193",
   "metadata": {},
   "source": [
    "## 以下是 Task1 和 Task3 的主要代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12eb9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from argparse import ArgumentParser\n",
    "from itertools import permutations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 实现一个基本的 Transformer 模块\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal transformer block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_mask = torch.full(\n",
    "            (len(x), len(x)), -float(\"Inf\"), device=x.device, dtype=x.dtype\n",
    "        )\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        x = self.ln_1(x)\n",
    "        a, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Transformer decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_heads=4, num_tokens=41, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(Block(dim, num_heads))\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_tokens, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20edefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成二元加法运算 mod p 的数据集\n",
    "def binary_plus_mod_p_data(p, eq_token, op_token):\n",
    "    \"\"\"\n",
    "    x ◦ y = x + y (mod p) for 0 ≤ x < p, 0 ≤ y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    result = (x + y) % p\n",
    "    \n",
    "    return torch.stack([x, op, y, eq, result])\n",
    "\n",
    "# 生成二元除法运算 mod p 的数据集\n",
    "def division_mod_p_data(p, eq_token, op_token):\n",
    "    \"\"\"\n",
    "    x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(1, p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    \n",
    "    # 由于取模意义下的除法可表示为乘上其的逆，故此处采用乘法进行操作\n",
    "    result = (x * y) % p   \n",
    "\n",
    "    return torch.stack([x, op, y, eq, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eab4511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2307/2307 [25:44<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "\n",
    "    model = Decoder(\n",
    "        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n",
    "    ).to(device)\n",
    "\n",
    "    # 生成数据\n",
    "    data = division_mod_p_data(args.p, eq_token, op_token)\n",
    "    train_ratio = 0.5               # 训练集比例,通过调整生成不同 training fraction 的结果\n",
    "    split = int(data.shape[1] * train_ratio)\n",
    "    idx_ = torch.randperm(data.shape[1])\n",
    "    train_idx, valid_idx = idx_[:split],idx_[split:]\n",
    "    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n",
    "\n",
    "    # For most experiments we used AdamW optimizer with learning rate 10−3,\n",
    "    # weight decay 1, β1 = 0.9, β2 = 0.98\n",
    "    optimizer = getattr(torch.optim, args.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr = args.lr,\n",
    "        weight_decay = args.weight_decay,\n",
    "        betas = (args.beta1, args.beta2),\n",
    "    )\n",
    "\n",
    "    #  linear learning rate warmup over the first 10 updates\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_data.shape[1] / args.batch_size)\n",
    "\n",
    "    train_acc, val_acc, train_loss, val_loss = [], [], [], []\n",
    "\n",
    "    for e in tqdm(range(int(args.budget) // steps_per_epoch)):\n",
    "\n",
    "        # randomly shuffle train data\n",
    "        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n",
    "\n",
    "        for data, is_train in [(train_data, True), (valid_data, False)]:\n",
    "\n",
    "            model.train(is_train)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            # torch.split faster than dataloader with tensor\n",
    "            dl = torch.split(data, args.batch_size, dim=1)\n",
    "            for input in dl:\n",
    "                input = input.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits = model(input[:-1])\n",
    "                    # calculate loss only on the answer part of the equation (last element\n",
    "                    loss = F.cross_entropy(logits[-1], input[-1])\n",
    "                    total_loss += loss.item() * input.shape[-1]\n",
    "\n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "                total_acc += acc.item() * input.shape[-1]\n",
    "\n",
    "            if is_train:\n",
    "                train_acc.append(total_acc / train_data.shape[-1])\n",
    "                train_loss.append(total_loss / train_data.shape[-1])\n",
    "            else:\n",
    "                val_acc.append(total_acc / valid_data.shape[-1])\n",
    "                val_loss.append(total_loss / valid_data.shape[-1])\n",
    "\n",
    "        if (e + 1) % 100 == 0:\n",
    "            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n",
    "            plt.plot(steps, train_acc, label=\"train\")\n",
    "            plt.plot(steps, val_acc, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 50% of data)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            \n",
    "            # 在以下代码中，修改为自己的图像存储路径\n",
    "            plt.savefig(\"figures-Task3/acc_weight_decay.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(steps, train_loss, label=\"train\")\n",
    "            plt.plot(steps, val_loss, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 50% of data) \")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures-Task3/loss_weight_decay.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--p\", type = int, default = 41)\n",
    "    parser.add_argument(\"--budget\", type = int, default = 3e4)\n",
    "    \n",
    "    # 可以通过调整此处的默认参数以比较不同的 BatchSize\n",
    "    parser.add_argument(\"--batch_size\", type = int, default = 64)\n",
    "    \n",
    "    # 可以通过调整此处的默认参数以比较不同的学习率\n",
    "    parser.add_argument(\"--lr\", type = float, default = 1e-3)\n",
    "    \n",
    "    parser.add_argument(\"--beta1\", type = float, default = 0.9)\n",
    "    parser.add_argument(\"--beta2\", type = float, default = 0.98)\n",
    "    \n",
    "    # 可以通过调整此处的默认参数以比较不同的 Weight decay 默认设置为 1\n",
    "    parser.add_argument(\"--weight_decay\", type = float, default = 1)\n",
    "    \n",
    "    # 可以通过调整此处的默认参数以比较不同的优化器\n",
    "    parser.add_argument(\"--optimizer\", default = \"AdamW\")\n",
    "    \n",
    "    # 可以通过添加以下代码，以实现 dropout\n",
    "    # parser.add_argument(\"--dropout_prob\", type=float, default=0.1)\n",
    "    \n",
    "    args = parser.parse_args(args = [])\n",
    "    # 注意：如果此处出现代码报错，可以修改如下\n",
    "    # args = paraser.parse_args()\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1c922",
   "metadata": {},
   "source": [
    "## 以下是 Task2 的主要代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_tokens=97, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(dim, dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Final output layer\n",
    "        self.head = nn.Linear(dim, num_tokens, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding + positional encoding\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[1], device=x.device).unsqueeze(-1)  # (batch_size, seq_len)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        h, _ = self.lstm(h)\n",
    "\n",
    "        # Get logits for each token in the sequence\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcb125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP model for modular division.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, num_layers=2, num_tokens=97, seq_len=5):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_tokens, dim)\n",
    "        self.position_embeddings = nn.Embedding(seq_len, dim)\n",
    "        \n",
    "        # Create the layers for the MLP\n",
    "        mlp_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            mlp_layers.append(nn.Linear(dim, dim))\n",
    "            mlp_layers.append(nn.GELU())\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        self.head = nn.Linear(dim, num_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.token_embeddings(x)\n",
    "        positions = torch.arange(x.shape[0], device=x.device).unsqueeze(-1)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        \n",
    "        h = self.mlp(h)\n",
    "        logits = self.head(h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7328d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "\n",
    "    # Use LSTM-based Decoder\n",
    "    model = DecoderLSTM(dim=128, num_layers=2, num_tokens=args.p + 2, seq_len=5).to(device)\n",
    "\n",
    "    # Prepare data\n",
    "    data = division_mod_p_data(args.p, eq_token, op_token)\n",
    "    train_idx, valid_idx = torch.randperm(data.shape[1]).split(data.shape[1] // 2)\n",
    "    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n",
    "\n",
    "    optimizer = getattr(torch.optim, args.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "        betas=(args.beta1, args.beta2),\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_data.shape[1] / args.batch_size)\n",
    "\n",
    "    train_acc, val_acc, train_loss, val_loss = [], [], [], []\n",
    "\n",
    "    for e in tqdm(range(int(args.budget) // steps_per_epoch)):\n",
    "\n",
    "        # randomly shuffle train data\n",
    "        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n",
    "\n",
    "        for data, is_train in [(train_data, True), (valid_data, False)]:\n",
    "\n",
    "            model.train(is_train)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            dl = torch.split(data, args.batch_size, dim=1)\n",
    "            for input in dl:\n",
    "                input = input.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits = model(input[:-1])\n",
    "                    loss = F.cross_entropy(logits[-1], input[-1])\n",
    "                    total_loss += loss.item() * input.shape[-1]\n",
    "\n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "                total_acc += acc.item() * input.shape[-1]\n",
    "\n",
    "            if is_train:\n",
    "                train_acc.append(total_acc / train_data.shape[-1])\n",
    "                train_loss.append(total_loss / train_data.shape[-1])\n",
    "            else:\n",
    "                val_acc.append(total_acc / valid_data.shape[-1])\n",
    "                val_loss.append(total_loss / valid_data.shape[-1])\n",
    "\n",
    "        if (e + 1) % 100 == 0:\n",
    "            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n",
    "            plt.plot(steps, train_acc, label=\"train\")\n",
    "            plt.plot(steps, val_acc, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (LSTM-based Model)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures/acc.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(steps, train_loss, label=\"train\")\n",
    "            plt.plot(steps, val_loss, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (LSTM-based Model)\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures/loss.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--p\", type=int, default=97)\n",
    "    parser.add_argument(\"--budget\", type=int, default=3e5)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=512)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--beta1\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--beta2\", type=float, default=0.98)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0)\n",
    "    parser.add_argument(\"--optimizer\", default=\"Adam\")\n",
    "    args = parser.parse_args(args = [])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e02a80",
   "metadata": {},
   "source": [
    "## 以下是 Task4 的主要代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3c8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"由于上述代码中调用的 torch.cartesian_prod 函数仅支持计算两个张量的笛卡尔积，\n",
    "#  故我们首先实现一个函数用于计算多个张量的笛卡尔积,为防止递归调用 torch.cartesian_prod\" \n",
    "#  带来的内存开销,此处选择 网格数据 + 平展 的方式生成笛卡尔积.\" \n",
    "\n",
    "def cartesian_prod_multi(tensors_list):\n",
    "    \n",
    "    # 确保至少有两个张量用于计算笛卡尔积\n",
    "    if len(tensors_list) < 2:\n",
    "        raise ValueError(\"At least two tensors are required for Cartesian product.\")\n",
    "    \n",
    "    # 使用 torch.meshgrid 创建网格\n",
    "    mesh = torch.meshgrid(*tensors_list, indexing='ij')\n",
    "    \n",
    "    # 将每个网格展平并堆叠在一起\n",
    "    flattened = [m.flatten() for m in mesh]\n",
    "    \n",
    "    # 转置以获得正确的顺序，并将结果转换为单个张量\n",
    "    result = torch.stack(flattened, dim=1)\n",
    "    \n",
    "    return result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cfa3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对多元运算问题,生成训练和测试数据\n",
    "def multivariate_plus_mod_p_data(p , eq_token , op_token ,k):\n",
    "    \"\"\"\n",
    "    x_1 ◦ x_2 ◦ ... ◦ x_k = x_1 + x_2 + ... + x_k (mod p) for 0 ≤ x_i < p (i = 1,2,...,k)\n",
    "    \"\"\"\n",
    "    # 生成 k 个向量的笛卡尔积\n",
    "    tensor_list,stack_list = [],[]\n",
    "    for _ in range(k):\n",
    "        tensor_list.append(torch.arange(p))\n",
    "    combine_tensor = cartesian_prod_multi(tensor_list)\n",
    "\n",
    "    # 生成运算符的编码向量\n",
    "    eq = torch.ones((combine_tensor.shape[1],)) * eq_token\n",
    "    op = torch.ones((combine_tensor.shape[1],)) * op_token\n",
    "    \n",
    "    # 生成运算结果向量\n",
    "    result = torch.sum(combine_tensor,axis = 0) % p\n",
    "    \n",
    "    # 将所有编码向量按顺序组合\n",
    "    for i in range(combine_tensor.shape[0]):\n",
    "        stack_list.append(combine_tensor[i])\n",
    "        stack_list.append(op)\n",
    "    stack_list.append(eq)\n",
    "    stack_list.append(result)\n",
    "    \n",
    "    return torch.stack(stack_list).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f79c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                     | 20/174 [35:43<9:01:29, 210.97s/it]"
     ]
    }
   ],
   "source": [
    "# 与 Task1-Task3 的代码基本相同，只是添加了一个参数 args.k 表示多元运算的维度\n",
    "def main(args):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    eq_token = args.p\n",
    "    op_token = args.p + 1\n",
    "\n",
    "    model = Decoder(\n",
    "        dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len= 2 * args.k + 1\n",
    "    ).to(device)\n",
    "\n",
    "    data = multivariate_plus_mod_p_data(args.p, eq_token, op_token,args.k)\n",
    "    train_ratio = 0.8               # 训练集比例,通过调整生成不同 training fraction 的结果\n",
    "    split = int(data.shape[1] * train_ratio)\n",
    "    idx_ = torch.randperm(data.shape[1])\n",
    "    train_idx, valid_idx = idx_[:split],idx_[split:]\n",
    "    train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n",
    "\n",
    "    # For most experiments we used AdamW optimizer with learning rate 10−3,\n",
    "    # weight decay 1, β1 = 0.9, β2 = 0.98\n",
    "    optimizer = getattr(torch.optim, args.optimizer)(\n",
    "        model.parameters(),\n",
    "        lr = args.lr,\n",
    "        weight_decay = args.weight_decay,\n",
    "        betas = (args.beta1, args.beta2),\n",
    "    )\n",
    "\n",
    "    #  linear learning rate warmup over the first 10 updates\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, lambda update: 1 if update > 10 else update / 10\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = math.ceil(train_data.shape[1] / args.batch_size)\n",
    "\n",
    "    train_acc, val_acc, train_loss, val_loss = [], [], [], []\n",
    "\n",
    "    for e in tqdm(range(int(args.budget) // steps_per_epoch)):\n",
    "\n",
    "        # randomly shuffle train data\n",
    "        train_data = train_data[:, torch.randperm(train_data.shape[1])]\n",
    "\n",
    "        for data, is_train in [(train_data, True), (valid_data, False)]:\n",
    "\n",
    "            model.train(is_train)\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            # torch.split faster than dataloader with tensor\n",
    "            dl = torch.split(data, args.batch_size, dim=1)\n",
    "            for input in dl:\n",
    "                input = input.to(device)\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits = model(input[:-1])\n",
    "                    # calculate loss only on the answer part of the equation (last element\n",
    "                    loss = F.cross_entropy(logits[-1], input[-1])\n",
    "                    total_loss += loss.item() * input.shape[-1]\n",
    "\n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                acc = (logits[-1].argmax(-1) == input[-1]).float().mean()\n",
    "                total_acc += acc.item() * input.shape[-1]\n",
    "\n",
    "            if is_train:\n",
    "                train_acc.append(total_acc / train_data.shape[-1])\n",
    "                train_loss.append(total_loss / train_data.shape[-1])\n",
    "            else:\n",
    "                val_acc.append(total_acc / valid_data.shape[-1])\n",
    "                val_loss.append(total_loss / valid_data.shape[-1])\n",
    "\n",
    "        if (e + 1) % 10 == 0:\n",
    "            steps = torch.arange(len(train_acc)).numpy() * steps_per_epoch\n",
    "            plt.plot(steps, train_acc, label=\"train\")\n",
    "            plt.plot(steps, val_acc, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 80% of data) with p = 41 and K = 3\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures-Task4/acc_k_3_80.png\", dpi=150)  # 修改图像的存储路径\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(steps, train_loss, label=\"train\")\n",
    "            plt.plot(steps, val_loss, label=\"val\")\n",
    "            plt.legend()\n",
    "            plt.title(\"Modular Division (training on 80% of data) with p = 41 and K = 3\")\n",
    "            plt.xlabel(\"Optimization Steps\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.xscale(\"log\", base=10)\n",
    "            plt.savefig(\"figures-Task4/loss_k_3_80.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--p\", type = int, default = 41)\n",
    "    parser.add_argument(\"--budget\", type = int, default = 3e5)\n",
    "    parser.add_argument(\"--batch_size\", type = int, default = 32)\n",
    "    parser.add_argument(\"--lr\", type = float, default = 1e-3)\n",
    "    parser.add_argument(\"--beta1\", type = float, default = 0.9)\n",
    "    parser.add_argument(\"--beta2\", type = float, default = 0.98)\n",
    "    parser.add_argument(\"--weight_decay\", type = float, default = 1)\n",
    "    parser.add_argument(\"--optimizer\", default = \"AdamW\")\n",
    "    \n",
    "    # 多元运算的维度，默认设定为 k = 3，可以尝试更大的 K,但会受到内存的限制。\n",
    "    parser.add_argument(\"--k\", type = int, default = 3)\n",
    "    \n",
    "    args = parser.parse_args(args = [])\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036fb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
